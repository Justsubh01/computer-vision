{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Multi-Class Object Detection with Pytorch**\n\n---","metadata":{}},{"cell_type":"markdown","source":"![images](https://i.pinimg.com/originals/a7/a2/e1/a7a2e105d70e2a66117cf7a862d7bbb3.png)\n![images](https://i.pinimg.com/originals/cf/bc/fc/cfbcfcebd4cabca94912035985e954f3.png)","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **Table of contensts**\n\n* [Import utilities](#Import-utilities)\n\n* [Extract info. from xml files](#Extract-info.-from-xml-files)\n\n* [Make dataframe from extracted information](#Make-dataframe-from-extracted-information)\n\n* [Separate train and validation data](#Separate-train-and-validation-data)\n\n* [View sample](#View-sample)\n\n* [Download pretrained model](#Download-pretrained-model)\n\n* [Train object detection model](#Train-object-detection-model)\n\n* [Test model](#Test-model)","metadata":{}},{"cell_type":"markdown","source":"## **Import utilities**","metadata":{}},{"cell_type":"code","source":"import os\nimport collections\nimport pandas as pd\nimport numpy as np\nimport functools\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom sklearn import preprocessing \n\n\nimport xml.etree.ElementTree as ET\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import SequentialSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = \"../input/pascal-voc-2012/VOC2012\"\nXML_PATH = os.path.join(BASE_PATH, \"Annotations\")\nIMG_PATH = os.path.join(BASE_PATH, \"JPEGImages\")\nXML_FILES = [os.path.join(XML_PATH, f) for f in os.listdir(XML_PATH)]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Extract info. from xml files**","metadata":{}},{"cell_type":"code","source":"class XmlParser(object):\n\n    def __init__(self,xml_file):\n\n        self.xml_file = xml_file\n        self._root = ET.parse(self.xml_file).getroot()\n        self._objects = self._root.findall(\"object\")\n        # path to the image file as describe in the xml file\n        self.img_path = os.path.join(IMG_PATH, self._root.find('filename').text)\n        # image id \n        self.image_id = self._root.find(\"filename\").text\n        # names of the classes contained in the xml file\n        self.names = self._get_names()\n        # coordinates of the bounding boxes\n        self.boxes = self._get_bndbox()\n\n    def parse_xml(self):\n        \"\"\"\"Parse the xml file returning the root.\"\"\"\n    \n        tree = ET.parse(self.xml_file)\n        return tree.getroot()\n\n    def _get_names(self):\n\n        names = []\n        for obj in self._objects:\n            name = obj.find(\"name\")\n            names.append(name.text)\n\n        return np.array(names)\n\n    def _get_bndbox(self):\n\n        boxes = []\n        for obj in self._objects:\n            coordinates = []\n            bndbox = obj.find(\"bndbox\")\n            coordinates.append(np.int32(bndbox.find(\"xmin\").text))\n            coordinates.append(np.int32(np.float32(bndbox.find(\"ymin\").text)))\n            coordinates.append(np.int32(bndbox.find(\"xmax\").text))\n            coordinates.append(np.int32(bndbox.find(\"ymax\").text))\n            boxes.append(coordinates)\n\n        return np.array(boxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Make dataframe from extracted information**","metadata":{}},{"cell_type":"code","source":"def xml_files_to_df(xml_files):\n    \n    \"\"\"\"Return pandas dataframe from list of XML files.\"\"\"\n    \n    names = []\n    boxes = []\n    image_id = []\n    xml_path = []\n    img_path = []\n    for file in xml_files:\n        xml = XmlParser(file)\n        names.extend(xml.names)\n        boxes.extend(xml.boxes)\n        image_id.extend([xml.image_id] * len(xml.names))\n        xml_path.extend([xml.xml_file] * len(xml.names))\n        img_path.extend([xml.img_path] * len(xml.names))\n    a = {\"image_id\": image_id,\n         \"names\": names,\n         \"boxes\": boxes,\n         \"xml_path\":xml_path,\n         \"img_path\":img_path}\n    \n    df = pd.DataFrame.from_dict(a, orient='index')\n    df = df.transpose()\n    \n    return df\n\ndf = xml_files_to_df(XML_FILES)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# check values for per class\ndf['names'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove .jpg extension from image_id \ndf['img_id'] = df['image_id'].apply(lambda x:x.split('.')).map(lambda x:x[0])\ndf.drop(columns=['image_id'], inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classes need to be in int form so we use LabelEncoder for this task\nenc = preprocessing.LabelEncoder()\ndf['labels'] = enc.fit_transform(df['names'])\ndf['labels'] = np.stack(df['labels'][i]+1 for i in range(len(df['labels']))) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = df[['names','labels']].value_counts()\nclasses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make dictionary for class objects so we can call objects by their keys.\nclasses= {1:'aeroplane',2:'bicycle',3:'bird',4:'boat',5:'bottle',6:'bus',7:'car',8:'cat',9:'chair',10:'cow',11:'diningtable',12:'dog',13:'horse',14:'motorbike',15:'person',16:'pottedplant',17:'sheep',18:'sofa',19:'train',20:'tvmonitor'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bounding box coordinates point need to be in separate columns\n\ndf['xmin'] = -1\ndf['ymin'] = -1\ndf['xmax'] = -1\ndf['ymax'] = -1\n\ndf[['xmin','ymin','xmax','ymax']]=np.stack(df['boxes'][i] for i in range(len(df['boxes'])))\n\ndf.drop(columns=['boxes'], inplace=True)\ndf['xmin'] = df['xmin'].astype(np.float)\ndf['ymin'] = df['ymin'].astype(np.float)\ndf['xmax'] = df['xmax'].astype(np.float)\ndf['ymax'] = df['ymax'].astype(np.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop names column since we dont need it anymore\ndf.drop(columns=['names'], inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df['img_id'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Separate train and validation data**","metadata":{}},{"cell_type":"code","source":"image_ids = df['img_id'].unique()\nvalid_ids = image_ids[-4000:]\ntrain_ids = image_ids[:-4000]\nlen(train_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df = df[df['img_id'].isin(valid_ids)]\ntrain_df = df[df['img_id'].isin(train_ids)]\nvalid_df.shape, train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Make dataset by Dataset Module** ","metadata":{}},{"cell_type":"code","source":"class VOCDataset(Dataset):\n    \n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n        \n        self.image_ids = dataframe['img_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n    \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['img_id'] == image_id]\n        \n        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        rows, cols = image.shape[:2]\n        \n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        \n       \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        label = records['labels'].values\n        labels = torch.as_tensor(label, dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)\n            \n            return image, target\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform_train():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.2),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format':'pascal_voc', 'label_fields': ['labels']})\n\ndef get_transform_valid():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = VOCDataset(train_df, IMG_PATH , get_transform_train())\nvalid_dataset = VOCDataset(valid_df, IMG_PATH, get_transform_valid())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **View sample**","metadata":{}},{"cell_type":"code","source":"images, targets= next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nplt.figure(figsize=(20,20))\nfor i, (image, target) in enumerate(zip(images, targets)):\n    plt.subplot(2,2, i+1)\n    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    names = targets[i]['labels'].cpu().numpy().astype(np.int64)\n    for i,box in enumerate(boxes):\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (0, 0, 220), 2)\n        cv2.putText(sample, classes[names[i]], (box[0],box[1]+15),cv2.FONT_HERSHEY_COMPLEX ,0.5,(0,220,0),1,cv2.LINE_AA)  \n\n    plt.axis('off')\n    plt.imshow(sample)\n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download pretrained model","metadata":{}},{"cell_type":"code","source":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 21  \n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.001,momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Download modules for model training**","metadata":{}},{"cell_type":"code","source":"!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pytorch/vision.git\n!cd vision;cp references/detection/utils.py ../;cp references/detection/transforms.py ../;cp references/detection/coco_eval.py ../;cp references/detection/engine.py ../;cp references/detection/coco_utils.py ../","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://i.pinimg.com/originals/cf/bc/fc/cfbcfcebd4cabca94912035985e954f3.png## **Train object detection model**","metadata":{}},{"cell_type":"code","source":"# let's train it for 1 epochs\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, valid_data_loader, device=device)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'faster_rcnn_state.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Test model**","metadata":{}},{"cell_type":"code","source":"# load  a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n\nWEIGHTS_FILE = \"./faster_rcnn_state.pth\"\n\nnum_classes = 21\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the traines weights\nmodel.load_state_dict(torch.load(WEIGHTS_FILE))\n\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def obj_detector(img):\n    img = cv2.imread(img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n\n\n    img /= 255.0\n    img = torch.from_numpy(img)\n    img = img.unsqueeze(0)\n    img = img.permute(0,3,1,2)\n    \n    model.eval()\n\n    detection_threshold = 0.70\n    \n    img = list(im.to(device) for im in img)\n    output = model(img)\n\n    for i , im in enumerate(img):\n        boxes = output[i]['boxes'].data.cpu().numpy()\n        scores = output[i]['scores'].data.cpu().numpy()\n        labels = output[i]['labels'].data.cpu().numpy()\n\n        labels = labels[scores >= detection_threshold]\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    \n    sample = img[0].permute(1,2,0).cpu().numpy()\n    sample = np.array(sample)\n    boxes = output[0]['boxes'].data.cpu().numpy()\n    name = output[0]['labels'].data.cpu().numpy()\n    scores = output[0]['scores'].data.cpu().numpy()\n    boxes = boxes[scores >= detection_threshold].astype(np.int32)\n    names = name.tolist()\n    \n    return names, boxes, sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_path = \"../input/data-images\"\npred_files = [os.path.join(pred_path,f) for f in os.listdir(pred_path)]\n\nplt.figure(figsize=(20,60))\nfor i, images in enumerate(pred_files):\n    if i > 19:break\n    plt.subplot(10,2,i+1)\n    names,boxes,sample = obj_detector(images)\n    for i,box in enumerate(boxes):\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (0, 220, 0), 2)\n        cv2.putText(sample, classes[names[i]], (box[0],box[1]-5),cv2.FONT_HERSHEY_COMPLEX ,0.7,(220,0,0),1,cv2.LINE_AA)  \n\n    plt.axis('off')\n    plt.imshow(sample)\n#     plt.savefig('save_image.png', bbox_inches='tight')  # if you want to save result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# **Thats all folks,please consider uplvote this notebook, Thanks for your time.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}